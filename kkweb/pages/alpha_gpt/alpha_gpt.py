import os
import streamlit as st
import logging
import time
from loguru import logger
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_community.adapters.openai import convert_openai_messages
import json

# Set up OpenAI API key
os.environ['OPENAI_API_KEY'] = 'sk-H3MkimLzxvofEhZOfGKTveqFqDqfSa6p5vqsHwZv6lkAlLE5'
KEY = os.environ['OPENAI_API_KEY']

# Initialize OpenAI client
client = OpenAI(
    api_key=KEY,
    base_url="https://api.moonshot.cn/v1",
)

# Streamlit page configuration
st.set_page_config(page_title="Alpha GPT", page_icon="ğŸ§ ", layout="wide")
st.title("Alpha GPT")

# Initialize chat history in session state if not already present
if "messages" not in st.session_state:
    st.session_state.messages = []

# Define the FactorGPTAgent class
class FactorGPTAgent:
    def __init__(self, user_prompt):
        self.sources = self.read_file_2_list('./agents/worldquant_101.txt')
        self.user_prompt = user_prompt
        optional_params = {
            "response_format": {"type": "json_object"}
        }
        self.model = ChatOpenAI(
            temperature=0,
            openai_api_key=KEY.strip(),
            model='moonshot-v1-8k',
            base_url="https://api.moonshot.cn/v1",
            max_retries=1,
            model_kwargs=optional_params
        )

    def read_file_2_list(self, filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f.readlines()]

    def build_prompt(self):
        prompt = [{
            "role": "system",
            "content": (
                "ä½ æ˜¯ä¸€ä¸ªé‡åŒ–åˆ†æå¸ˆ. ä½ å¯ä»¥é€šè¿‡é˜…è¯»å¤šä¸ªalphaå› å­è¡¨è¾¾å¼ï¼Œæ€»ç»“å…¶å†…åœ¨è§„å¾‹ï¼Œå¹¶ä¸”å¯ä»¥åˆ›æ–°æ€§çš„ç”Ÿæˆå¯ç”¨çš„å› å­è¡¨è¾¾å¼ã€‚"
                "å¯¹äºç”Ÿæˆçš„è¡¨è¾¾å¼ï¼Œä½ èƒ½å¤Ÿè§£é‡Šå…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”¨æ¸…æ™°ç®€æ´çš„è¯­è¨€è§£é‡Šå…¶å„ä¸ªå˜é‡çš„å«ä¹‰ã€‚\n"
            )
        }, {
            "role": "user",
            "content": (
                f"ç”¨æˆ·æŒ‡ä»¤æè¿°: {self.user_prompt}\n"
                f"æ ·ä¾‹æ•°æ®ï¼š{self.sources}\n"
                f"ä½ çš„ä»»åŠ¡æ˜¯å­¦ä¹ ä»¥ä¸Šèµ„æºä¹‹åï¼Œæ€»ç»“å…¶è§„å¾‹ï¼Œè¾“å‡ºä¸€ä¸ªåŒç±»å‹è¡¨è¾¾å¼ã€‚\n"
                f"ä¹¦å†™è¡¨è¾¾å¼æ—¶ï¼Œè¯·ä»…ä½¿ç”¨æ ·ä¾‹æ•°æ®é‡Œçš„å‡½æ•°ï¼Œæ¯æ¬¡ç”Ÿæˆ1ä¸ªï¼Œç”Ÿæˆçš„è¡¨è¾¾å¼ï¼Œä¸è¦å¸¦Alpha#xxx,æœŸæœ›å› å­çš„ç›¸å…³æ€§ä½\n"
                f"Please return nothing but a JSON in the following format:\n"
                f'{{"expr": "ç”Ÿæˆçš„å› å­è¡¨è¾¾å¼", "desc": "å¯¹è¯¥å› å­è¡¨è¾¾å¼çš„è§£é‡Šè¯´æ˜"}}\n'
            )
        }]
        return prompt

    def run(self):
        lc_messages = convert_openai_messages(self.build_prompt())
        response = self.model.invoke(lc_messages).content
        return json.loads(response)

# Define a function to handle chat response generation and streaming
def generate_response(messages):
    try:
        response = client.chat.completions.create(
            model="moonshot-v1-8k",
            messages=messages,
            temperature=0.3,
            stream=True,
        )

        collected_messages = []
        response_placeholder = st.empty()

        for idx, chunk in enumerate(response):
            chunk_message = chunk.choices[0].delta
            if not chunk_message.content:
                continue
            collected_messages.append(chunk_message)
            response_text = ''.join([m.content for m in collected_messages])
            response_placeholder.write(response_text)

        return response_text

    except Exception as e:
        logging.error(f"Error during response generation: {str(e)}")
        raise e

# Function to stream FactorGPTAgent response
def stream_factor_response(user_prompt):
    factor_agent = FactorGPTAgent(user_prompt)
    response = factor_agent.run()
    response_message = f"å› å­è¡¨è¾¾å¼: {response['expr']}\n\nè§£é‡Š: {response['desc']}"
    response_placeholder = st.empty()
    for line in response_message.split('\n'):
        response_placeholder.write(line)
        time.sleep(0.1)
    return response_message

# Main function for Streamlit app
def main():
    st.title("Chat with LLMs Models")
    st.sidebar.write("### General Guidelines")
    st.sidebar.write(
        """
        - Start your query with a clear question or statement.
        - Be specific about what you need.
        - You can ask follow-up questions based on previous answers.
        """
    )

    logger.info("App started")

    # Sidebar for model selection
    model = st.sidebar.selectbox("Choose a model", ["moonshot-v1-8k"])
    logger.info(f"Model selected: {model}")

    # Prompt for user input and save to chat history
    if prompt := st.chat_input("Your question"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        logger.info(f"User input: {prompt}")

        # Display the user's query
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])

        # Generate a new response if the last message is not from the assistant
        if st.session_state.messages[-1]["role"] != "assistant":
            with st.chat_message("assistant"):
                start_time = time.time()
                logger.info("Generating response")

                with st.spinner("Writing..."):
                    try:
                        # Check if the query is related to "å› å­" (factor)
                        if "å› å­" in prompt:
                            response_message = stream_factor_response(prompt)
                        else:
                            # Generate response for the user's input
                            messages = [{"role": msg["role"], "content": msg["content"]} for msg in st.session_state.messages]
                            system_message = {
                                "role": "system",
                                "content": "ä½ æ˜¯ Kimiï¼Œç”± Moonshot AI æä¾›çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ æ›´æ“…é•¿ä¸­æ–‡å’Œè‹±æ–‡çš„å¯¹è¯ã€‚ä½ ä¼šä¸ºç”¨æˆ·æä¾›å®‰å…¨ï¼Œæœ‰å¸®åŠ©ï¼Œå‡†ç¡®çš„å›ç­”ã€‚åŒæ—¶ï¼Œä½ ä¼šæ‹’ç»ä¸€åˆ‡æ¶‰åŠææ€–ä¸»ä¹‰ï¼Œç§æ—æ­§è§†ï¼Œé»„è‰²æš´åŠ›ç­‰é—®é¢˜çš„å›ç­”ã€‚Moonshot AI ä¸ºä¸“æœ‰åè¯ï¼Œä¸å¯ç¿»è¯‘æˆå…¶ä»–è¯­è¨€ã€‚"
                            }
                            messages.insert(0, system_message)
                            response_message = generate_response(messages)
                        
                        duration = time.time() - start_time
                        response_message_with_duration = (
                            f"{response_message}\n\nDuration: {duration:.2f} seconds"
                        )
                        st.session_state.messages.append(
                            {"role": "assistant", "content": response_message_with_duration}
                        )
                        st.write(f"Duration: {duration:.2f} seconds")
                        logger.info(f"Response: {response_message}, Duration: {duration:.2f} s")

                    except Exception as e:
                        st.session_state.messages.append({"role": "assistant", "content": str(e)})
                        st.error("An error occurred while generating the response.")
                        logger.error(f"Error: {str(e)}")

# Run the app
if __name__ == "__main__":
    main()
